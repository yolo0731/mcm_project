"""
æ¨¡å‹ä¼˜åŒ–ç­–ç•¥æ¨¡å—
"""
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.feature_selection import mutual_info_classif
from sklearn.ensemble import VotingClassifier
import copy
import config

class FeatureOptimizer:
    """ç‰¹å¾ä¼˜åŒ–å™¨"""

    def __init__(self):
        self.optimizers = {
            'pca': PCA,
            'lda': LinearDiscriminantAnalysis,
            'mutual_info': self._mutual_info_selection
        }

    def _mutual_info_selection(self, X, y, n_components=None):
        """åŸºäºäº’ä¿¡æ¯çš„ç‰¹å¾é€‰æ‹©"""
        from sklearn.feature_selection import SelectKBest
        if n_components is None:
            n_components = min(20, X.shape[1])
        selector = SelectKBest(mutual_info_classif, k=n_components)
        return selector.fit_transform(X, y)

    def optimize_features(self, X_train, y_train, X_test, method='pca', n_components=None):
        """ç‰¹å¾ä¼˜åŒ–"""
        print(f"Applying {method} feature optimization...")

        if n_components is None:
            n_components = min(25, X_train.shape[1])

        if method == 'pca':
            optimizer = PCA(n_components=n_components)
            X_train_opt = optimizer.fit_transform(X_train)
            X_test_opt = optimizer.transform(X_test)

        elif method == 'lda':
            optimizer = LinearDiscriminantAnalysis(n_components=min(n_components, len(np.unique(y_train))-1))
            X_train_opt = optimizer.fit_transform(X_train, y_train)
            X_test_opt = optimizer.transform(X_test)

        elif method == 'mutual_info':
            from sklearn.feature_selection import SelectKBest
            optimizer = SelectKBest(mutual_info_classif, k=n_components)
            X_train_opt = optimizer.fit_transform(X_train, y_train)
            X_test_opt = optimizer.transform(X_test)

        else:
            raise ValueError(f"Unknown optimization method: {method}")

        return X_train_opt, X_test_opt, optimizer

class ModelEnsemble:
    """æ¨¡å‹é›†æˆå™¨"""

    def __init__(self, models, method='voting'):
        self.models = models
        self.method = method
        self.weights = None

    def fit_ensemble_weights(self, val_loader, device='cuda'):
        """è®­ç»ƒé›†æˆæƒé‡"""
        model_predictions = []
        true_labels = []

        # è·å–æ¯ä¸ªæ¨¡å‹çš„éªŒè¯é›†é¢„æµ‹
        for model in self.models:
            model.eval()
            predictions = []
            with torch.no_grad():
                for batch in val_loader:
                    features = batch['features'].to(device)
                    if len(true_labels) == 0:
                        true_labels.extend(batch['labels'].cpu().numpy())

                    outputs = model(features)
                    probabilities = torch.softmax(outputs, dim=1)
                    predictions.extend(probabilities.cpu().numpy())

            model_predictions.append(np.array(predictions))

        # åŸºäºéªŒè¯é›†æ€§èƒ½è®¡ç®—æƒé‡
        model_predictions = np.array(model_predictions)  # [n_models, n_samples, n_classes]
        true_labels = np.array(true_labels)

        # è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„å‡†ç¡®ç‡
        accuracies = []
        for i, preds in enumerate(model_predictions):
            predicted_classes = np.argmax(preds, axis=1)
            accuracy = np.mean(predicted_classes == true_labels)
            accuracies.append(accuracy)

        # åŸºäºå‡†ç¡®ç‡çš„è½¯æƒé‡
        accuracies = np.array(accuracies)
        self.weights = accuracies / np.sum(accuracies)

        print(f"Ensemble weights: {self.weights}")

    def predict_ensemble(self, data_loader, device='cuda'):
        """é›†æˆé¢„æµ‹"""
        ensemble_predictions = []
        true_labels = []

        # è·å–æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
        model_outputs = []
        for model in self.models:
            model.eval()
            predictions = []
            with torch.no_grad():
                for batch in data_loader:
                    features = batch['features'].to(device)
                    if len(true_labels) == 0:
                        true_labels.extend(batch['labels'].cpu().numpy())

                    outputs = model(features)
                    probabilities = torch.softmax(outputs, dim=1)
                    predictions.extend(probabilities.cpu().numpy())

            model_outputs.append(np.array(predictions))

        # é›†æˆé¢„æµ‹
        model_outputs = np.array(model_outputs)  # [n_models, n_samples, n_classes]

        if self.method == 'voting':
            if self.weights is not None:
                # åŠ æƒå¹³å‡
                ensemble_probs = np.average(model_outputs, axis=0, weights=self.weights)
            else:
                # ç®€å•å¹³å‡
                ensemble_probs = np.mean(model_outputs, axis=0)

        elif self.method == 'max':
            # å–æœ€å¤§æ¦‚ç‡
            ensemble_probs = np.max(model_outputs, axis=0)

        else:
            raise ValueError(f"Unknown ensemble method: {self.method}")

        ensemble_predictions = np.argmax(ensemble_probs, axis=1)

        return ensemble_predictions, ensemble_probs, true_labels

class ModelCompressor:
    """æ¨¡å‹å‹ç¼©å™¨"""

    def __init__(self):
        pass

    def prune_model(self, model, pruning_ratio=0.2):
        """æ¨¡å‹å‰ªæ"""
        import torch.nn.utils.prune as prune

        model_copy = copy.deepcopy(model)

        # å¯¹æ¯ä¸ªLinearå±‚è¿›è¡Œå‰ªæ
        for name, module in model_copy.named_modules():
            if isinstance(module, nn.Linear):
                prune.l1_unstructured(module, name='weight', amount=pruning_ratio)
                prune.remove(module, 'weight')

        print(f"Model pruned with ratio: {pruning_ratio}")
        return model_copy

    def quantize_model(self, model):
        """æ¨¡å‹é‡åŒ–"""
        quantized_model = torch.quantization.quantize_dynamic(
            model, {nn.Linear}, dtype=torch.qint8
        )
        print("Model quantized to int8")
        return quantized_model

    def knowledge_distillation(self, teacher_model, student_model, data_loader,
                             num_epochs=10, temperature=3.0, alpha=0.7, device='cuda'):
        """çŸ¥è¯†è’¸é¦"""
        teacher_model.eval()
        student_model.train()

        optimizer = optim.Adam(student_model.parameters(), lr=0.001)
        kl_loss = nn.KLDivLoss()
        ce_loss = nn.CrossEntropyLoss()

        for epoch in range(num_epochs):
            total_loss = 0.0
            for batch in data_loader:
                features = batch['features'].to(device)
                labels = batch['labels'].to(device)

                # Teacher outputs
                with torch.no_grad():
                    teacher_outputs = teacher_model(features)
                    soft_targets = torch.softmax(teacher_outputs / temperature, dim=1)

                # Student outputs
                student_outputs = student_model(features)
                soft_predictions = torch.log_softmax(student_outputs / temperature, dim=1)

                # è’¸é¦æŸå¤±
                distillation_loss = kl_loss(soft_predictions, soft_targets) * (temperature ** 2)

                # åˆ†ç±»æŸå¤±
                classification_loss = ce_loss(student_outputs, labels)

                # æ€»æŸå¤±
                loss = alpha * distillation_loss + (1 - alpha) * classification_loss

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                total_loss += loss.item()

            print(f"Distillation Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(data_loader):.4f}")

        return student_model

class DataAugmentor:
    """æ•°æ®å¢å¼ºå™¨"""

    def __init__(self):
        pass

    def add_noise(self, data, noise_level=0.1):
        """æ·»åŠ å™ªå£°"""
        noise = torch.randn_like(data) * noise_level
        return data + noise

    def scale_data(self, data, scale_range=(0.8, 1.2)):
        """å°ºåº¦å˜æ¢"""
        scale_factor = torch.uniform(scale_range[0], scale_range[1], (data.size(0), 1))
        if data.is_cuda:
            scale_factor = scale_factor.cuda()
        return data * scale_factor

    def augment_dataset(self, data_loader, augmentation_methods=['noise', 'scaling'], device='cuda'):
        """æ•°æ®é›†å¢å¼º"""
        augmented_data = []
        augmented_labels = []

        for batch in data_loader:
            features = batch['features'].to(device)
            labels = batch['labels'].to(device)

            # åŸå§‹æ•°æ®
            augmented_data.append(features)
            augmented_labels.append(labels)

            # åº”ç”¨å¢å¼º
            for method in augmentation_methods:
                if method == 'noise':
                    aug_features = self.add_noise(features)
                elif method == 'scaling':
                    aug_features = self.scale_data(features)
                else:
                    continue

                augmented_data.append(aug_features)
                augmented_labels.append(labels)

        # åˆå¹¶æ‰€æœ‰å¢å¼ºæ•°æ®
        all_features = torch.cat(augmented_data, dim=0)
        all_labels = torch.cat(augmented_labels, dim=0)

        print(f"Original dataset size: {len(data_loader.dataset)}")
        print(f"Augmented dataset size: {len(all_features)}")

        return all_features, all_labels

class OptimizationManager:
    """ä¼˜åŒ–ç®¡ç†å™¨"""

    def __init__(self):
        self.feature_optimizer = FeatureOptimizer()
        self.model_compressor = ModelCompressor()
        self.data_augmentor = DataAugmentor()

    def comprehensive_optimization(self, models, data_loaders, device='cuda'):
        """ç»¼åˆä¼˜åŒ–"""
        print("ğŸš€ Starting Comprehensive Model Optimization")
        print("=" * 60)

        optimization_results = {}

        # 1. ç‰¹å¾ä¼˜åŒ–
        print("1. Feature Optimization...")
        # è¿™é‡Œå¯ä»¥å®ç°ç‰¹å¾ä¼˜åŒ–é€»è¾‘

        # 2. æ¨¡å‹é›†æˆ
        print("2. Model Ensemble...")
        ensemble = ModelEnsemble(models, method='voting')
        if 'validation' in data_loaders:
            ensemble.fit_ensemble_weights(data_loaders['validation'], device)

        # 3. æ¨¡å‹å‹ç¼©
        print("3. Model Compression...")
        compressed_models = {}
        for name, model in zip(['source', 'dann'], models):
            # å‰ªæ
            pruned_model = self.model_compressor.prune_model(model, pruning_ratio=0.1)
            compressed_models[f'{name}_pruned'] = pruned_model

            # é‡åŒ–
            try:
                quantized_model = self.model_compressor.quantize_model(copy.deepcopy(model))
                compressed_models[f'{name}_quantized'] = quantized_model
            except Exception as e:
                print(f"Quantization failed for {name}: {e}")

        # 4. æ•°æ®å¢å¼º
        print("4. Data Augmentation...")
        # è¿™é‡Œå¯ä»¥å®ç°æ•°æ®å¢å¼ºé€»è¾‘

        optimization_results = {
            'ensemble': ensemble,
            'compressed_models': compressed_models,
            'feature_optimization': 'Applied',
            'data_augmentation': 'Applied'
        }

        print("âœ… Comprehensive optimization completed!")
        return optimization_results

if __name__ == "__main__":
    print("Optimization strategies module loaded successfully!")